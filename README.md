Word2Vec is a popular technique in natural language processing (NLP) used to represent words as vectors in a continuous vector space. Developed by a team of researchers at Google led by Tomas Mikolov, Word2Vec can capture the semantic meaning of words based on their contexts.
There are two main architectures in Word2Vec: Continuous Bag of Words (CBOW) and Skip-gram.

CBOW predicts the current word based on the context words within a window, while Skip-grams use the current word to predict the surrounding window of context words.
Both CBOW and Skip-grams are model architectures used in Word2Vec, a popular technique for learning word embeddings by training a neural network on a large corpus of text.
